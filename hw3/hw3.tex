%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Homework 3} % Assignment title
\newcommand{\hmwkDueDate}{July 28,\ 2014} % Due date
\newcommand{\hmwkClass}{Probability} % Course/class
\newcommand{\hmwkClassTime}{6:00 pm} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Instructor: Elena Kosygina} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Weiyi Chen} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

%\newpage
%\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    \begin{homeworkSection}{Find the distribution function of $X$}
        Denote $Y$ as Jereny's medical expenses in one year, which have exponential distribution with mean 1, which implies that
        \begin{equation}
             \begin{split}
                 f(y; \lambda = 1) &= 
                 \begin{cases}
                    \lambda e^{-\lambda y}, &\text{ if }y \ge 0 \\
                    \lambda 0, &\text{ if }y < 0
                 \end{cases} =
                 \begin{cases}
                     e^{-y}, &\text{ if }y \ge 0\\
                     0, &\text{ if }y < 0
                 \end{cases}\\
                 F(y; \lambda = 1) &=
                 \begin{cases}
                     1-e^{-\lambda y}, &\text{ if }y \ge 0\\
                     0, &\text{ if }y < 0
                 \end{cases} =
                 \begin{cases}
                     1-e^{-y}, &\text{ if }y \ge 0\\
                     0, &\text{ if }y < 0
                 \end{cases}
             \end{split}
        \end{equation} 
        where $\lambda = 1^{-1} = 1$. \\
        Now we consider $X$, $X$ cannot have density since
        \begin{equation}
            P(X>1) = 0
        \end{equation}
        because the maximum of the amount of unreimbursed medical expenses is $1 + 20\% \times (5-1) = 1.8$ (in thousand of dollars). Moreover, $X$ is not simple as it can take any value in $[0,1.8]$. \\
        Let us find $F_X(x) = P(X \le x)$. If $x < 0$,
        \begin{equation}
            F_X(x) = 0
        \end{equation}
        If $0 \le x < 1$, that is equivalent to $0 \le y < 1$ since $X = 1$ in this interval
        \begin{equation}
            F_X(x) = P(X \le x) = P(Y \le 1) = F_Y(1) = 1 - e^{-1}
        \end{equation}
        If $1 \le x < 1.8$, that is equivalent to $1 \le y < 5$ since $X = 1+ 0.2(Y-1) = 0.8+0.2Y$ in this interval
        \begin{equation}
            F_X(x) = P(X \le x) = P(0.8+0.2Y \le x) = F_Y(5x-4) = 1 - e^{-(5x-4)}
        \end{equation}
        If $x \ge 1$,
        \begin{equation}
            F_X(x) = P(X \le x) = 1
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{Find the distribution measure of $X$}
        Note that there is a jump in $F_X(x)$ at $x = 1$, since 
        \begin{equation}
            \lim_{x\to1^{-}} F_X(x) = 1 - e^{-5}, \lim_{x\to1^{+}} F_X(x) = 1  
        \end{equation}
        where the gap is $e^{-5}$. \\
        Distribution measure of $X$ is
        \begin{equation}
            \mu_X = \delta_0 + \mu_0 + e^{-5}\delta_1
        \end{equation}
        where
        \begin{equation}
            \begin{split}
                \mu_0([a,b]) &= \int_a^b 1_{[1,1.8]}(x)f(x)dx \\
                \mu_X([a,b]) &= \delta_0([a,b]) \int_a^b 1_{[1,1.8]}(x)f(x)dx + e^{-5}\delta_1([a,b])
            \end{split}
        \end{equation}
        where $f(x) = 5e^{-(5x-4)}$.
    \end{homeworkSection}
    \begin{homeworkSection}{Find $E(X)$}
        \begin{equation}
            EX = \delta_0(R) + \int_1^{1.8} xf(x)dx + e^{-5}\delta_1(R) = 1 + \int_1^{1.8} 5xe^{-(5x-4)}dx + e^{-5} = 0.6e^{-5}-1.6e^{-1}+1.8 = 1.2154
        \end{equation}
        thousand dollars.
    \end{homeworkSection}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 2
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    The distribution of $X$ under $\tilde P$ is
    \begin{equation}
        \tilde F_X(x) = \int_{\Omega} I_{X(\omega) \le x} Z(\omega) d\omega = \int_{-\infty}^x f(x)\frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx
    \end{equation}
    where $f(x)$ is given as
    \begin{equation}
        f(x) = e^{-\frac{(x+\theta)^2}{2} + \frac{x^2}{2}}
    \end{equation}
    Then
    \begin{equation}
        \tilde F_X(x) = \int_{\infty}^{x} \frac{1}{\sqrt{2\pi}} e^{\frac{(x+\theta)^2}{2}} dx
    \end{equation}
    Therefore, under $\tilde P$, $X \sim N(-\theta, 1)$.
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 3
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    For any $u \in R$,
    \begin{equation}
        \begin{split}
            \tilde E(e^{uY}) &= E(e^{uY}Z) = E(e^{u(X+\theta)}e^{-\theta X - \frac{1}{2}\theta^2}) \\
            &= E(e^{(u-\theta)X}) e^{u\theta - \frac{1}{2}\theta^2} = e^{\frac{1}{2}(u-\theta)^2} e^{u\theta - \frac{1}{2}\theta^2} \\
            &= e^{u^2/2}
        \end{split}
    \end{equation}
    Therefore $Y$ is standard normal under $\tilde P$.
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 4
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    \begin{homeworkSection}{(i)}
        \begin{equation}
            \frac{1}{\epsilon} P(X \in B(x,\epsilon)) = \frac{1}{\epsilon} \frac{1}{\sqrt{2\pi}} \int_{x-\epsilon}^{x+\epsilon} e^{-u^2/2}du \approx \frac{1}{\epsilon} \frac{1}{\sqrt{2\pi}} e^{x^2/2} \epsilon = \frac{1}{\sqrt{2\pi}} e^{x^2/2}
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{(ii)}
        \begin{equation}
            \frac{1}{\epsilon} P(Y \in B(y,\epsilon)) = \frac{1}{\epsilon} \frac{1}{\sqrt{2\pi}} \int_{y-\epsilon}^{y+\epsilon} e^{-u^2/2}du \approx \frac{1}{\epsilon} \frac{1}{\sqrt{2\pi}} e^{y^2/2} \epsilon = \frac{1}{\sqrt{2\pi}} e^{y^2/2}
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{(iii)}
        As given
        \begin{equation}
            \begin{split}
                \{X \in B(x, \epsilon)\} &= \{w: x-\epsilon/2 \le X(w) \le x+\epsilon/2 \} \\
                \{Y \in B(y, \epsilon)\} &= \{w: y-\epsilon/2 \le Y(w) \le y+\epsilon/2 \}
            \end{split}
        \end{equation}
        By substituting $y$ with $x+\theta$ and $Y(w)= X(w) + \theta$, we find the second formula becomes
        \begin{equation}
             \{w: x+\theta-\epsilon/2 \le X(w)+\theta \le x+\theta+\epsilon/2 \} = \{w: x-\epsilon/2 \le X(w) \le x+\epsilon/2 \}
        \end{equation} 
        Therefore they are the same set.
    \end{homeworkSection}
    \begin{homeworkSection}{(iv)}
        \begin{equation}
            \begin{split}
                \frac{\tilde P(A)}{P(A)} &\approx \exp[-Y^2(\overline w)/2 + X^2(\overline w)/2] \\
                &= \exp[-(X(\overline w)+\theta)^2/2 + X^2(\overline w)/2] \\
                &= \exp[-\theta X(\overline w) - \theta^2/2]
            \end{split}
        \end{equation}
    \end{homeworkSection}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 5
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    \begin{homeworkSection}{Poisson distribution}
        A discrete random variable $X$ is said to have a Poisson distribution with parameter $\lambda > 0$, if, for $k =0,1,2,\dots$ the probability mass function of $X$ is given by:
        \begin{equation}
            \!f_{P}(k; \lambda)= \Pr_{P}(X=k)= \frac{\lambda^k e^{-\lambda}}{k!},
        \end{equation}
        where
        \begin{itemize}
            \item e is Euler's number (e = 2.71828...)
            \item k! is the factorial of k.
        \end{itemize}
        The positive real number $\lambda$ is equal to the expected value of X and also to its variance.
        \begin{equation}
            \lambda=\operatorname{E}(X)=\operatorname{Var}(X).
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{Binomial distribution}
        In general, if the random variable $X$ follows the binomial distribution with parameters $n$ and $p$, we write $X \sim B(n, p)$. The probability of getting exactly $k$ successes in $n$ trials is given by the probability mass function:
        \begin{equation}
            f_B(k;n,p) = \Pr_B(X = k) = {n\choose k}p^k(1-p)^{n-k}
        \end{equation}
        for $k = 0, 1, 2, \dots, n$, where
        \begin{equation}
            {n\choose k}=\frac{n!}{k!(n-k)!}
        \end{equation}
        is the binomial coefficient.
    \end{homeworkSection}
    \begin{homeworkSection}{Distribution of $S$ and $F$}
        The probability of getting exactly $k$ successes in $X$ trials is given by the probability mass function:
        \begin{equation}
            \Pr(S = k | X) = f_B(k;X,p)
        \end{equation}
        Therefore
        \begin{equation}
            \begin{split}
                \Pr(S = k) &= \sum_{i=k}^{\infty} f_P(i; \lambda) f_B(k;i,p) \\
                &= \sum_{i=k}^{\infty} \Pr_{P}(X=i)\Pr_B(S=k) \\
                &= \sum_{i=k}^{\infty} \frac{\lambda^i e^{-\lambda}}{i!} {i\choose k}p^k(1-p)^{i-k} \\
                &= \sum_{i=k}^{\infty} \frac{\lambda^i e^{-\lambda}}{i!} \frac{i!}{k!(i-k)!} p^k(1-p)^{i-k} \\
                &= \frac{(p\lambda)^k e^{-\lambda}}{k!} \sum_{i=k}^{\infty} \frac{\lambda^{i-k}(1-p)^{i-k}}{(i-k)!}
            \end{split}
        \end{equation}
        where I didn't consider $i < k$ because it is impossible to generate $k$ successes with $i < k$ trials. We note that the last term is exactly the taylor series of exponential function, that is
        \begin{equation}
            \sum_{i=k}^{\infty} \frac{\lambda^{i-k}(1-p)^{i-k}}{(i-k)!} = e^{(1-p)\lambda}
        \end{equation}
        Therefore,
        \begin{equation}
            \Pr(S = k) = \frac{(p\lambda)^k e^{-\lambda}}{k!} e^{(1-p)\lambda} = \frac{(p\lambda)^ke^{-p\lambda}}{k!}
        \end{equation}
        which implies that $S \sim \text{Poisson}(p\lambda)$. \\
        Similarly we can substitute $(1-p)$ with $p$ to find the distribution of $F$, follow the same process and conclude that $F \sim \text{Poisson}((1-p)\lambda)$.
    \end{homeworkSection}
    \begin{homeworkSection}{Independence of $S$ and $F$}
        For any positive integers $k_1$ and $k_2$,
        \begin{equation}
            \begin{split}
                \Pr(S = k_1) &= f_{P}(k_1; p\lambda) = \frac{(p\lambda)^{k_1} e^{-p\lambda}}{(k_1)!}, \\
                \Pr(F = k_2) &= f_{P}(k_2; (1-p)\lambda) = \frac{[(1-p)\lambda]^{k_2} e^{-(1-p)\lambda}}{(k_2)!}
            \end{split}
        \end{equation}
        For the event $S=k_1, F=k_2$, the number of Bernoulli trials is exactly $k_1 + k_2$, then
        \begin{equation}
            \Pr(S=k_1, F=k_2) = f_P(k_1+k_2; \lambda) f_B(k_1;k_1+k_2,p) = \frac{\lambda^{k_1+k_2} e^{-\lambda}}{(k_1+k_2)!}p^{k_1}(1-p)^{k_2} {k_1+k_2\choose k_1}
        \end{equation}
        Since
        \begin{equation}
            \frac{e^{-p\lambda}}{(k_1)!} \frac{e^{-(1-p)\lambda}}{(k_2)!} = \frac{e^{-\lambda}}{k_1!k_2!} = \frac{e^{-\lambda}(k_1+k_2)!}{k_1!k_2!(k_1+k_2)!} = e^{-\lambda}  \frac{1}{(k_1+k_2)!} {k_1+k_2\choose k_1}
        \end{equation}
        We have exactly
        \begin{equation}
            \Pr(S=k_1, F=k_2) = \Pr(S = k_1) \Pr(F = k_2)
        \end{equation}
        So we can conclude $S$ and $F$ are independent.
    \end{homeworkSection}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 6
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    Let $\cal{G}$ be generated by the partition $E_1 = [0,1/3), E_2 = \{1/3\}, E_3 = (1/3, 1/2), E_4 = [1/2, 1]$, then $\cal{G}$ is the power set of them. Now let us compute $E(X|\cal{G})$.
    \begin{homeworkSection}{(i) $X(w) = 1_{(0,1/4)}(w)$}
        \begin{equation}
            \begin{split}
                c_1 &= \frac{\int_{E_1} X dP}{P(E_1)} = \frac{1/4}{1/3} = \frac{3}{4} \\
                c_2 &= \frac{\int_{E_2} X dP}{P(E_2)} = 0 \\
                c_3 &= \frac{\int_{E_3} X dP}{P(E_3)} = 0 \\
                c_4 &= \frac{\int_{E_4} X dP}{P(E_4)} = 0
            \end{split}
        \end{equation}
        For any other elements $E_i$ of $\cal{G}$,
        \begin{equation}
            c_i = \sum_{n=1}^4 1_{E_n \subset E_i} c_n
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{(ii) $X(w) = w$}
        \begin{equation}
            \begin{split}
                c_1 &= \frac{\int_{E_1} X dP}{P(E_1)} = \frac{1/18}{1/3} = \frac{1}{6} \\
                c_2 &= \frac{\int_{E_2} X dP}{P(E_2)} = 0 \\
                c_3 &= \frac{\int_{E_3} X dP}{P(E_3)} = \frac{5/72}{1/6} = \frac{5}{12} \\
                c_4 &= \frac{\int_{E_4} X dP}{P(E_4)} = \frac{3}{4}
            \end{split}
        \end{equation}
        For any other elements $E_i$ of $\cal{G}$,
        \begin{equation}
            c_i = \sum_{n=1}^4 1_{E_n \subset E_i} c_n
        \end{equation}
    \end{homeworkSection}
    \begin{homeworkSection}{(ii) $X(w) = (w-1/2)^2$}
        \begin{equation}
            \begin{split}
                c_1 &= \frac{\int_{E_1} X dP}{P(E_1)} = \frac{13}{108} \\
                c_2 &= \frac{\int_{E_2} X dP}{P(E_2)} = 0 \\
                c_3 &= \frac{\int_{E_3} X dP}{P(E_3)} = \frac{1}{108} \\
                c_4 &= \frac{\int_{E_4} X dP}{P(E_4)} = \frac{1}{12}
            \end{split}
        \end{equation}
        For any other elements $E_i$ of $\cal{G}$,
        \begin{equation}
            c_i = \sum_{n=1}^4 1_{E_n \subset E_i} c_n
        \end{equation}
    \end{homeworkSection}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 7
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    Recall that $W_n:=X_1+X_2+\dots+X_n$ and $R_n:=\frac{X_2+\dots+X_n}{X1}$ are independent. This is well-known and can be proved by the usual method of derived distributions for two random variables. the mapping $(X_1,X_2+X_3+\dots+X_n) \to (R_n,W_n)$ is $1-1$ and easily inverted. \\
    Then, noting that $\frac{X_1}{W_n}=\frac{1}{1+R_n}$,
    \begin{equation}
        E(X_1|W_n) = E(\frac{W_n}{1+R_n}|W_n) = W_nE(\frac{1}{1+R_n}|W_n) = W_n E(\frac{1}{1+R_n})
    \end{equation}
    The last equality follows from independence. Also, on repeating the argument, or just taking unconditional expectations,
    \begin{equation}
        EX_1 = E(\frac{W_n}{1+R_n}) = E(\frac{1}{1+R_n}) EW_n = E\frac{1}{1+R_n} nE(X_1)
    \end{equation}
    So
    \begin{equation}
        E\frac{1}{1+R_n} = \frac{1}{n}
    \end{equation}
    which can be plugged to get the result,
    \begin{equation}
        E(X_1|W_n) = \frac{W_n}{n}
    \end{equation}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%   PROBLEM 8
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
    Since $X$ and $Y$ are standard normal random variables, then $\mu_1 = \mu_2 =0 ,\sigma_1 = \sigma_2 =1$, and the parameters of $W$ is
    \begin{equation}
        \begin{split}
            \mu_3 &= EW = \mu_2 - \frac{\rho \sigma_2}{\sigma_1} \mu_1 = 0 \\
            \sigma_3 &= \sqrt{Var(W)} = \sqrt{1 - \rho^2}
        \end{split}
    \end{equation}
    Therefore,
    \begin{equation}
        \begin{split}
            g(x) &= Ef(x, \rho x + W) \\
            &= \frac{1}{\sigma_3\sqrt{2\pi \sigma_3}} \int_{-\infty}^{\infty} f(x,\rho x + w) \exp(-\frac{w^2}{2\sigma_3^2}) dw
        \end{split}
    \end{equation}
    where
    \begin{equation}
        f(x,\rho x + w) = \exp(x+\rho x+ w) = \exp((1+\rho)x + w)
    \end{equation}
    So,
    \begin{equation}
        \begin{split}
            g(x) &= \frac{1}{\sigma_3\sqrt{2\pi \sigma_3}} \int_{-\infty}^{\infty} \exp[(1+\rho)x + w -\frac{w^2}{2\sigma_3^2}] dw \\
            &= \frac{1}{\sigma_3\sqrt{2\pi \sigma_3}} \int_{-\infty}^{\infty} \exp[\frac{-(w-\sigma_3^2)^2 + 2(1+\rho)\sigma_3^2 x + \sigma_3^4}{2\sigma_3^2}] dw\\
            &= \frac{\exp[(1+\rho)x + \sigma_3^2/2]}{\sqrt{\sigma_3}} \int_{-\infty}^{\infty}\frac{1}{\sigma_3\sqrt{2\pi}} e^{-\frac{(w-\sigma_3^2)^2}{2\sigma_3^2}} dw \\
            &= \frac{\exp[(1+\rho)x + \sigma_3^2/2]}{\sqrt{\sigma_3}}
        \end{split}
    \end{equation}
    Substitute $\sigma_3$, we have
    \begin{equation}
        g(x) = \frac{\exp[(1+\rho)x + (1-\rho^2)/2]}{(1-\rho^2)^{1/4}}
    \end{equation}
    Then
    \begin{equation}
        E(f(X,Y)|X) = g(X) = \frac{\exp[(1+\rho)X + (1-\rho^2)/2]}{(1-\rho^2)^{1/4}}
    \end{equation}
\end{homeworkProblem}

\end{document}